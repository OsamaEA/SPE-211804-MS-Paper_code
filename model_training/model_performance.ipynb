{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G7nFS1mRgzVW",
    "outputId": "a5df4636-09e6-4af1-ebc9-8e567c54f7da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'yolov5'...\n",
      "remote: Enumerating objects: 15110, done.\u001b[K\n",
      "remote: Counting objects: 100% (65/65), done.\u001b[K\n",
      "remote: Compressing objects: 100% (55/55), done.\u001b[K\n",
      "remote: Total 15110 (delta 29), reused 27 (delta 10), pack-reused 15045\u001b[K\n",
      "Receiving objects: 100% (15110/15110), 14.13 MiB | 35.37 MiB/s, done.\n",
      "Resolving deltas: 100% (10346/10346), done.\n",
      "/content/yolov5\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.0/184.0 KB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cvxpy 1.2.3 requires setuptools<=64.0.2, but you have setuptools 67.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.0/49.0 KB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 KB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 KB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Setup complete. Using torch 1.13.1+cu116 (CPU)\n"
     ]
    }
   ],
   "source": [
    "# Importing primary modules\n",
    "import sys\n",
    "sys.path.insert(0, './model_data')\n",
    "from google.colab import files\n",
    "import os\n",
    "import numpy as np\n",
    "from IPython.display import Image, clear_output  # to display images\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "#clone YOLOv5 and \n",
    "!git clone https://github.com/ultralytics/yolov5  # clone repo\n",
    "%cd yolov5\n",
    "%pip install -qr requirements.txt # install dependencies\n",
    "\n",
    "# Prinitng current device properties\n",
    "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation on our Datasets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VBCq1f5ZxCZR",
    "outputId": "52ee2f9a-8e17-449e-9378-5e0d94037f51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mdata=/content/datasets/trial_data-9/data.yaml, weights=['runs/train/exp/weights/best.pt'], batch_size=32, imgsz=416, conf_thres=0.001, iou_thres=0.65, task=train, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=True, dnn=False\n",
      "YOLOv5 🚀 v6.1-316-g916bdb1 Python-3.7.13 torch-1.12.0+cu113 CUDA:0 (Tesla T4, 15110MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 213 layers, 7023610 parameters, 0 gradients, 15.8 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/datasets/trial_data-9/train/labels.cache' images and labels... 3668 found, 0 missing, 0 empty, 0 corrupt: 100% 3668/3668 [00:00<?, ?it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 115/115 [00:39<00:00,  2.90it/s]\n",
      "                 all       3668       4873      0.923      0.907      0.957       0.68\n",
      "              coning       3668        433       0.87      0.833      0.894      0.576\n",
      "            constant       3668        757      0.944      0.979      0.989      0.778\n",
      "          multilayer       3668       1447      0.925      0.927       0.97      0.689\n",
      "              normal       3668        921      0.965      0.955      0.989      0.777\n",
      "               rapid       3668       1315      0.913      0.842      0.942      0.581\n",
      "Speed: 0.1ms pre-process, 1.8ms inference, 1.4ms NMS per image at shape (32, 3, 416, 416)\n",
      "Results saved to \u001b[1mruns/val/exp4\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python val.py --weights runs/train/exp/weights/best.pt --data {dataset.location}/data.yaml --img 416 --iou 0.65 --half --task 'train'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7_9kUOK4xCdg",
    "outputId": "77729fa2-26e6-4849-eff4-df0994ad7422"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mdata=/content/datasets/trial_data-9/data.yaml, weights=['runs/train/exp/weights/best.pt'], batch_size=32, imgsz=416, conf_thres=0.001, iou_thres=0.65, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=True, dnn=False\n",
      "YOLOv5 🚀 v6.1-316-g916bdb1 Python-3.7.13 torch-1.12.0+cu113 CUDA:0 (Tesla T4, 15110MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 213 layers, 7023610 parameters, 0 gradients, 15.8 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/datasets/trial_data-9/valid/labels.cache' images and labels... 1079 found, 0 missing, 0 empty, 0 corrupt: 100% 1079/1079 [00:00<?, ?it/s]\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 34/34 [00:14<00:00,  2.42it/s]\n",
      "                 all       1079       1414      0.786      0.772      0.815       0.53\n",
      "              coning       1079        143      0.704      0.533      0.563      0.299\n",
      "            constant       1079        231      0.832      0.922      0.915      0.668\n",
      "          multilayer       1079        403      0.721       0.79      0.821      0.504\n",
      "              normal       1079        273      0.874      0.861      0.927      0.698\n",
      "               rapid       1079        364      0.797      0.755      0.847      0.478\n",
      "Speed: 0.1ms pre-process, 2.0ms inference, 1.6ms NMS per image at shape (32, 3, 416, 416)\n",
      "Results saved to \u001b[1mruns/val/exp6\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python val.py --weights runs/train/exp/weights/best.pt --data {dataset.location}/data.yaml --img 416 --iou 0.65 --half --task 'val'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C) Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jc_dFuDVAuUA",
    "outputId": "710552ed-fdd2-49cb-b5d1-eed3fef33533"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mdata=/content/datasets/trial_data-9/data.yaml, weights=['runs/train/exp/weights/best.pt'], batch_size=32, imgsz=416, conf_thres=0.001, iou_thres=0.65, task=test, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=True, dnn=False\n",
      "YOLOv5 🚀 v6.1-316-g916bdb1 Python-3.7.13 torch-1.12.0+cu113 CUDA:0 (Tesla T4, 15110MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 213 layers, 7023610 parameters, 0 gradients, 15.8 GFLOPs\n",
      "\u001b[34m\u001b[1mtest: \u001b[0mScanning '/content/datasets/trial_data-9/test/labels' images and labels...536 found, 0 missing, 0 empty, 0 corrupt: 100% 536/536 [00:00<00:00, 1362.38it/s]\n",
      "\u001b[34m\u001b[1mtest: \u001b[0mNew cache created: /content/datasets/trial_data-9/test/labels.cache\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 17/17 [00:08<00:00,  2.00it/s]\n",
      "                 all        536        713       0.82      0.772      0.848      0.564\n",
      "              coning        536         62      0.668      0.552      0.669      0.413\n",
      "            constant        536        117      0.874      0.915      0.942        0.7\n",
      "          multilayer        536        221      0.826      0.816      0.855      0.527\n",
      "              normal        536        123      0.869      0.846      0.923      0.669\n",
      "               rapid        536        190      0.864      0.733       0.85      0.512\n",
      "Speed: 0.1ms pre-process, 2.4ms inference, 2.1ms NMS per image at shape (32, 3, 416, 416)\n",
      "Results saved to \u001b[1mruns/val/exp7\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python val.py --weights runs/train/exp/weights/best.pt --data {dataset.location}/data.yaml --img 416 --iou 0.65 --half --task 'test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNWZ7SMhjmwf"
   },
   "source": [
    "# Extracting Data From Graphs into Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "id": "OXMtxg9Xjmz8",
    "outputId": "e05f0a71-fa11-447d-8ac9-2c72380b3bfc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 🚀 v7.0-94-g5c3eba6 Python-3.8.10 torch-1.13.1+cu116 CPU\n",
      "\n",
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/usr/local/lib/python3.8/dist-packages/urllib3-1.24.3.dist-info/METADATA'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model summary: 213 layers, 7023610 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-009769ab-025c-4d1e-a774-d3621bb0bad6\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>confidence</th>\n",
       "      <th>class</th>\n",
       "      <th>name</th>\n",
       "      <th>start_days</th>\n",
       "      <th>end_days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>307.928894</td>\n",
       "      <td>3.084538</td>\n",
       "      <td>394.377655</td>\n",
       "      <td>573.616638</td>\n",
       "      <td>0.876915</td>\n",
       "      <td>1</td>\n",
       "      <td>constant</td>\n",
       "      <td>180</td>\n",
       "      <td>1260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>417.869446</td>\n",
       "      <td>2.197540</td>\n",
       "      <td>478.772064</td>\n",
       "      <td>571.485107</td>\n",
       "      <td>0.773125</td>\n",
       "      <td>2</td>\n",
       "      <td>multilayer</td>\n",
       "      <td>2130</td>\n",
       "      <td>8280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-009769ab-025c-4d1e-a774-d3621bb0bad6')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-009769ab-025c-4d1e-a774-d3621bb0bad6 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-009769ab-025c-4d1e-a774-d3621bb0bad6');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "         xmin      ymin        xmax        ymax  confidence  class  \\\n",
       "0  307.928894  3.084538  394.377655  573.616638    0.876915      1   \n",
       "1  417.869446  2.197540  478.772064  571.485107    0.773125      2   \n",
       "\n",
       "         name  start_days  end_days  \n",
       "0    constant         180      1260  \n",
       "1  multilayer        2130      8280  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.hub.load('/content/yolov5', 'custom', path='/content/best.pt', source='local')  # local repository\n",
    "# converting img colors type\n",
    "img = cv2.imread('path_to_image')[:, :, ::-1]\n",
    "# interpreting results from the model\n",
    "results = model(img, size=416)\n",
    "# Identifying boundaries of bounding boxes for each interpreted class\n",
    "boxes = results.pandas().xyxy[0]\n",
    "# relating box position to log-log axes\n",
    "boxes['start_days'] = (round(10**(((boxes['xmin'] - 72.5) / (518.5 - 72.5) * (np.log10(20000) - np.log10(1))) + np.log10(1))/30)*30).astype(int)\n",
    "boxes['end_days'] = (round(10**(((boxes['xmax'] - 72.5) / (518.5 - 72.5) * (np.log10(20000) - np.log10(1))) + np.log10(1))/30)*30).astype(int)\n",
    "# visualizing the output\n",
    "boxes"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "liajFkrrsxMS"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
